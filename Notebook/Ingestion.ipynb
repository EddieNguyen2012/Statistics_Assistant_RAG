{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Set Up AstraDB to store docs\n",
    "\n",
    "I want to utilize AstraDB as the DB for storing documents as the docs are pretty large."
   ],
   "id": "660a42d23399045"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T22:29:52.736382Z",
     "start_time": "2026-02-06T22:29:51.679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from astrapy import DataAPIClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "ENDPOINT = os.getenv(\"ASTRA_DB_API_ENDPOINT\")\n",
    "TOKEN = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
    "# Initialize the client\n",
    "\n",
    "client = DataAPIClient()\n",
    "db = client.get_database(\n",
    "  ENDPOINT,\n",
    "  token=TOKEN,\n",
    ")\n",
    "print(f\"Connected to Astra DB: {db.name()}\")\n",
    "print(f\"Collections: {db.list_collection_names()}\")\n"
   ],
   "id": "13c3428b9aa4dd9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Astra DB: stat_rag_docs\n",
      "Collections: []\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Processor Construction",
   "id": "c22a1195e10ff95f"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-07T00:22:50.449715Z",
     "start_time": "2026-02-07T00:22:49.828380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from logging import raiseExceptions\n",
    "from langchain_astradb import AstraDBLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "doc_path = '../RAG_Docs'"
   ],
   "id": "a2bb7162b6785b7e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T06:26:36.882767Z",
     "start_time": "2026-02-07T06:26:36.833643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "# Helped by ChatGPT to generalize cleaning steps\n",
    "\n",
    "def clean_encoding(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f]', '', text)\n",
    "    return text\n",
    "\n",
    "def frequent(lines, pages, freq_threshold=0.7):\n",
    "    counts = Counter(l.strip().lower() for l in lines if l.strip())\n",
    "    return {\n",
    "        l for l, c in counts.items()\n",
    "        if c / len(pages) >= freq_threshold\n",
    "    }\n",
    "\n",
    "def strip_headers_footers(docs, top_n=2, bottom_n=2, freq_threshold=0.7):\n",
    "    pages = [d.page_content.splitlines() for d in docs]\n",
    "\n",
    "    top_lines, bottom_lines = [], []\n",
    "    for lines in pages:\n",
    "        top_lines.extend(lines[:top_n])\n",
    "        bottom_lines.extend(lines[-bottom_n:])\n",
    "\n",
    "    bad_top = frequent(top_lines, pages=pages, freq_threshold=freq_threshold)\n",
    "    bad_bottom = frequent(bottom_lines, pages=pages, freq_threshold=freq_threshold)\n",
    "\n",
    "    for d in docs:\n",
    "        lines = d.page_content.splitlines()\n",
    "        lines = [\n",
    "            l for l in lines\n",
    "            if l.strip().lower() not in bad_top\n",
    "            and l.strip().lower() not in bad_bottom\n",
    "        ]\n",
    "        d.page_content = \"\\n\".join(lines)\n",
    "\n",
    "    return docs\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    text = re.sub(r'(?:\\n\\s*){3,}', '\\n\\n', text)\n",
    "    text = re.sub(r'\\.{4,}', '.', text) # Avoiding ... case\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_docs(docs):\n",
    "    for d in docs:\n",
    "        d.page_content = clean_encoding(d.page_content)\n",
    "\n",
    "    docs = strip_headers_footers(docs)\n",
    "\n",
    "    for d in docs:\n",
    "        d.page_content = normalize_whitespace(d.page_content)\n",
    "\n",
    "    return docs\n",
    "\n",
    "# My work\n",
    "\n",
    "def path_validate(path):\n",
    "    if not os.path.exists(path):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "class DocIngestion:\n",
    "    def __init__(self, docs_path: str, chunk_size=100, chunk_overlap=20):\n",
    "        self.docs_path = docs_path if path_validate(docs_path) else raiseExceptions\n",
    "        self.chunk_size = -1\n",
    "        self.chunk_overlap = -1\n",
    "        self.splitter = None\n",
    "        self.update_splitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    def batch_ingest(self):\n",
    "        for file in [f for f in os.listdir(self.docs_path) if not f.startswith('.')]: # Get all files name for ingestion\n",
    "            res = self.individual_ingest(file)\n",
    "            return res\n",
    "        return None\n",
    "\n",
    "    # Return True if the file is loaded successfully, False otherwise\n",
    "    def individual_ingest(self, filename: str):\n",
    "        if path_validate(os.path.join(self.docs_path, filename)):\n",
    "            loader = PyPDFLoader(os.path.join(self.docs_path, filename))\n",
    "            document = loader.load()\n",
    "            cleaned_doc = preprocess_docs(document)\n",
    "            return self.chunking(cleaned_doc)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def chunking(self, doc):\n",
    "        chunks = self.splitter.split_documents(doc)\n",
    "        valid_chunks = [doc for doc in chunks if len(doc.page_content) > 50]\n",
    "        print(f\"Found total {len(chunks)} chunks\")\n",
    "        print(f\"Found {len(valid_chunks)} valid (len > 50) chunks\")\n",
    "        print(f\"Average chunk size: {np.mean([len(doc.page_content) for doc in valid_chunks])}\")\n",
    "        return valid_chunks\n",
    "\n",
    "    def update_splitter(self, chunk_size=-1, chunk_overlap=-1):\n",
    "\n",
    "        if chunk_size == -1 and chunk_overlap == -1:\n",
    "            print(\"Nothing to update\")\n",
    "            return\n",
    "\n",
    "        new_size = chunk_size if chunk_size != -1 else self.chunk_size\n",
    "        new_overlap = chunk_overlap if chunk_overlap != -1 else self.chunk_overlap\n",
    "\n",
    "        if new_size <= new_overlap:\n",
    "            print(f\"Error: Chunk size ({new_size}) must be greater than chunk overlap ({new_overlap}). Update aborted.\")\n",
    "            return\n",
    "\n",
    "        self.chunk_size = new_size\n",
    "        self.chunk_overlap = new_overlap\n",
    "\n",
    "        try:\n",
    "            self.splitter = RecursiveCharacterTextSplitter(\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "                chunk_size=self.chunk_size,\n",
    "                chunk_overlap=self.chunk_overlap,\n",
    "            )\n",
    "            print(f\"Current splitter: name={self.splitter.__class__}, chunk_size={self.chunk_size}, chunk_overlap={self.chunk_overlap}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error updating splitter: {e}\")"
   ],
   "id": "4ff361a22c249563",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T06:27:14.803726Z",
     "start_time": "2026-02-07T06:27:14.777440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ingestor = DocIngestion(\"../RAG_Docs\", chunk_size=600, chunk_overlap=100)\n",
    "# chunks = ingestor.individual_ingest(\"assumptions.pdf\")\n",
    "\n",
    "def test_chunking(filename):\n",
    "    book = ingestor.individual_ingest(filename)\n",
    "    for i in range(5):\n",
    "        content = book[i].page_content\n",
    "        print(f\"=============== Chunk {i + 1} ({len(content)} characters) ==============\")\n",
    "        print(content)\n",
    "        print(\"\\n\\n\")"
   ],
   "id": "e19e6b11c08a8e66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current splitter: name=<class 'langchain_text_splitters.character.RecursiveCharacterTextSplitter'>, chunk_size=600, chunk_overlap=100\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T06:27:15.978888Z",
     "start_time": "2026-02-07T06:27:15.333540Z"
    }
   },
   "cell_type": "code",
   "source": "test_chunking(\"assumptions.pdf\")",
   "id": "cfbb41f85db4ce0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found total 185 chunks\n",
      "Found 184 valid (len > 50) chunks\n",
      "Average chunk size: 464.5054347826087\n",
      "=============== Chunk 1 (283 characters) ==============\n",
      "Copyright @c 2012 by G. David Garson and Statistical Associates Publishing Page 1 \n",
      " \n",
      "Statistical \n",
      "Associates \n",
      "Publishing \n",
      "____________ \n",
      "Blue Book Series \n",
      "\n",
      "TESTING STATISTICAL ASSUMPTIONS \n",
      "By G. David Garson \n",
      "North Carolina State University \n",
      "School of Public and International Affairs\n",
      "\n",
      "\n",
      "\n",
      "=============== Chunk 2 (81 characters) ==============\n",
      "Copyright @c 2012 by G. David Garson and Statistical Associates Publishing Page 2\n",
      "\n",
      "\n",
      "\n",
      "=============== Chunk 3 (508 characters) ==============\n",
      "@c 2012 by G. David Garson and Statistical Associates Publishing. All rights reserved worldwide in all \n",
      "media. \n",
      "The author and publisher of this eBook and accompanying materials make no representation or \n",
      "warranties with respect to the accuracy, applicability, fitness, or completeness of the contents of this \n",
      "eBook or accompanying materials. The author and publisher disclaim any warranties (express or \n",
      "implied), merchantability, or fitness for any particular purpose. The author and publisher shall in no\n",
      "\n",
      "\n",
      "\n",
      "=============== Chunk 4 (517 characters) ==============\n",
      "event be held liable to any party for any direct, indirect, punitive, special, incidental or other \n",
      "consequential damages arising directly or indirectly from any use of this material, which is provided “as \n",
      "is”, and without warranties. Further, the author and publisher do not warrant the performance, \n",
      "effectiveness or applicability of any sites listed or linked to in this eBook or accompanying materials. All \n",
      "links are for information purposes only and are not warranted for content, accuracy or any other implied\n",
      "\n",
      "\n",
      "\n",
      "=============== Chunk 5 (425 characters) ==============\n",
      "or explicit purpose. This eBook and accompanying materials is © copyrighted by G. David Garson and \n",
      "Statistical Associates Publishing. No part of this may be copied, or changed in any format, sold, or used \n",
      "in any way under any circumstances. \n",
      "Contact: \n",
      "G. David Garson, President \n",
      "Statistical Publishing Associates \n",
      "274 Glenn Drive \n",
      "Asheboro, NC 27205 USA \n",
      " \n",
      "Email: gdavidgarson@gmail.com \n",
      "Web: www.statisticalassociates.com\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T06:27:19.863352Z",
     "start_time": "2026-02-07T06:27:17.536740Z"
    }
   },
   "cell_type": "code",
   "source": "test_chunking(\"BayesianGroupModeling.pdf\")",
   "id": "b70b83dcb2a36604",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found total 172 chunks\n",
      "Found 172 valid (len > 50) chunks\n",
      "Average chunk size: 540.0290697674419\n",
      "=============== Chunk 1 (588 characters) ==============\n",
      "Bayesian model selection for group studies — Revisited\n",
      "L. Rigoux a, K.E. Stephan b,c,K . J .F r i s t o nb,J .D a u n i z e a ua,b,⁎\n",
      "a Brain and Spine Institute, Paris, France\n",
      "b Wellcome Trust Centre for Neuroimaging, University College London, UK\n",
      "c Translational Neuromodeling Unit (TNU), Institute for Biomedical Engineering, University of Zurich & ETH Zurich, Switzerland\n",
      "abstractarticle info\n",
      "Article history:\n",
      "Accepted 29 August 2013\n",
      "Available online 7 September 2013\n",
      "Keywords:\n",
      "Statistical risk\n",
      "Exceedance probability\n",
      "Between-condition comparison\n",
      "Between-group comparison\n",
      "Mixed effects\n",
      "\n",
      "\n",
      "\n",
      "=============== Chunk 2 (535 characters) ==============\n",
      "Exceedance probability\n",
      "Between-condition comparison\n",
      "Between-group comparison\n",
      "Mixed effects\n",
      "Random effects\n",
      "DCM\n",
      "In this paper, we revisit the problem of Bayesian model selection (BMS) at the group level. We originally\n",
      "addressed this issue in Stephan et al. (2009) , where models are treated as random effects that could differ\n",
      "between subjects, with an unknown population distribution. Here, we extend this work, by (i) introducing\n",
      "the Bayesian omnibus risk (BOR) as a measure of the st atistical risk incurred when performing group BMS,\n",
      "\n",
      "\n",
      "\n",
      "=============== Chunk 3 (531 characters) ==============\n",
      "(ii) highlighting the difference between random effects BMS and classical random effects analyses of\n",
      "parameter estimates, and (iii) addressing the problem of between group or condition model comparisons.\n",
      "We address thefirst issue by quantifying the chance likelihood of apparent differences in model frequencies. This\n",
      "leads to the notion of protected exceedance probabilities. The second issue arises when people want to ask\n",
      "“whether a model parameter is zero or not ” at the group level. Here, we provide guidance as to whether to\n",
      "\n",
      "\n",
      "\n",
      "=============== Chunk 4 (550 characters) ==============\n",
      "use a classical second-level analysis of parameter estimates, or random effects BMS. The third issue rests on\n",
      "the evidence for a difference in model labels or frequencies across groups or conditions. Overall, we hope that\n",
      "the material presented in this paper finesses the problems of group-level BMS in the analysis of neuroimaging\n",
      "and behavioural data.\n",
      "© 2013 Elsevier Inc. All rights reserved.\n",
      "Introduction\n",
      "Any statistical measure of empirical evidence rests on some form of\n",
      "model comparison. In a classical setting, one typically compares the null\n",
      "\n",
      "\n",
      "\n",
      "=============== Chunk 5 (588 characters) ==============\n",
      "model comparison. In a classical setting, one typically compares the null\n",
      "with an alternative hypothesis, where the former is a model of how\n",
      "chance could have generated the data. Theoretical results specify the\n",
      "s e n s ei nw h i c hm o d e lc o m p a r i s o nc a nb ec o n s i d e r e do p t i m a l .F o r\n",
      "example, the Neyman–Pearson lemma essentially states that statistical\n",
      "tests based on the likelihood ratio (such as a simple t-test) are the\n",
      "most powerful, i.e., they have the best chance of detecting an effect\n",
      "(see e.g., Casella and Berger, 2001 ). From this perspective, Bayesian\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T06:27:21.525395Z",
     "start_time": "2026-02-07T06:27:21.200684Z"
    }
   },
   "cell_type": "code",
   "source": "test_chunking(\"ProcessModeling-NISTHandbook.pdf\")",
   "id": "e8783f93e83ff6fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found total 192 chunks\n",
      "Found 192 valid (len > 50) chunks\n",
      "Average chunk size: 496.859375\n",
      "=============== Chunk 1 (596 characters) ==============\n",
      "4. Process Modeling\n",
      "http://www.itl.nist.gov/div898/handbook/pmd/pmd.htm[6/27/2012 2:16:03 PM]\n",
      " \n",
      "4. Process Modeling\n",
      "The goal for this chapter is to present the background and specific analysis\n",
      "techniques needed to construct a statistical model that describes a particular\n",
      "scientific or engineering process. The types of models discussed in this\n",
      "chapter are limited to those based on an explicit mathematical function.\n",
      "These types of models can be used for prediction of process outputs, for\n",
      "calibration, or for process optimization.\n",
      "1. Introduction\n",
      "1. Definition\n",
      "2. Terminology\n",
      "3. Uses\n",
      "4. Methods\n",
      "\n",
      "\n",
      "\n",
      "=============== Chunk 2 (551 characters) ==============\n",
      "1. Introduction\n",
      "1. Definition\n",
      "2. Terminology\n",
      "3. Uses\n",
      "4. Methods\n",
      "2. Assumptions\n",
      "1. Assumptions\n",
      "3. Design\n",
      "1. Definition\n",
      "2. Importance\n",
      "3. Design Principles\n",
      "4. Optimal Designs\n",
      "5. Assessment\n",
      "4. Analysis\n",
      "1. Modeling Steps\n",
      "2. Model Selection\n",
      "3. Model Fitting\n",
      "4. Model Validation\n",
      "5. Model Improvement\n",
      "5. Interpretation & Use\n",
      "1. Prediction\n",
      "2. Calibration\n",
      "3. Optimization\n",
      "6. Case Studies\n",
      "1. Load Cell Output\n",
      "2. Alaska Pipeline\n",
      "3. Ultrasonic Reference Block\n",
      "4. Thermal Expansion of Copper\n",
      "Detailed Table of Contents: Process Modeling\n",
      "References: Process Modeling\n",
      "\n",
      "\n",
      "\n",
      "=============== Chunk 3 (126 characters) ==============\n",
      "Detailed Table of Contents: Process Modeling\n",
      "References: Process Modeling\n",
      "Appendix: Some Useful Functions for Process Modeling\n",
      "\n",
      "\n",
      "\n",
      "=============== Chunk 4 (568 characters) ==============\n",
      "4. Process Modeling\n",
      "http://www.itl.nist.gov/div898/handbook/pmd/pmd_d.htm[6/27/2012 2:16:30 PM]\n",
      " \n",
      "4. Process Modeling - Detailed Table of Contents [4.]\n",
      "The goal for this chapter is to present the background and specific analysis techniques needed to construct a\n",
      "statistical model that describes a particular scientific or engineering process. The types of models discussed in\n",
      "this chapter are limited to those based on an explicit mathematical function. These types of models can be used\n",
      "for prediction of process outputs, for calibration, or for process optimization.\n",
      "\n",
      "\n",
      "\n",
      "=============== Chunk 5 (568 characters) ==============\n",
      "for prediction of process outputs, for calibration, or for process optimization. \n",
      "1. Introduction to Process Modeling\n",
      " [4.1.]\n",
      "1. What is process modeling? [4.1.1.]\n",
      "2. What terminology do statisticians use to describe process models? [4.1.2.]\n",
      "3. What are process models used for? [4.1.3.]\n",
      "1. Estimation [4.1.3.1.]\n",
      "2. Prediction [4.1.3.2.]\n",
      "3. Calibration [4.1.3.3.]\n",
      "4. Optimization [4.1.3.4.]\n",
      "4. What are some of the different statistical methods for model building? [4.1.4.]\n",
      "1. Linear Least Squares Regression [4.1.4.1.]\n",
      "2. Nonlinear Least Squares Regression [4.1.4.2.]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "31d9e103ee0075d0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
